{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "OptCMDP Algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7QO_5_9DIoI"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import math\n",
        "import statistics as stat\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "from scipy.optimize import linprog"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNUXSJfwkORy"
      },
      "source": [
        "class MDP:\n",
        "    def __init__(self, numStates, numActions, p, c, d, alpha, stateDist=None):\n",
        "        \"\"\"temp[]\n",
        "        Reference:\n",
        "        Args:\n",
        "            numStates: integer, number of states of the MDP\n",
        "            numActions: integer, number of actions of the MDP\n",
        "            p: matrix, numStates-by-numActions-by-numStates probability \n",
        "                transition matrix of the MDP\n",
        "            r: matrix, numStates-by-numActions reward matrixof the MDP\n",
        "            stateDist: vector, 1-by-numState, the initial probability distribution\n",
        "                 over the states of the MDP, default is uniform distribution\n",
        "        Return:\n",
        "            A MDP object with the specified numStates, numActions, p, r\n",
        "            and stateDist\n",
        "        \"\"\"\n",
        "        \n",
        "        self.p = np.asarray(p)\n",
        "        self.c = c\n",
        "        self.d = d\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # The size of p is numStates by numActions by numStates\n",
        "        # p(s, a, s') denotes the probability to transit to state\n",
        "        # s' after taking action a at state s.\n",
        "        # As such, \\sum_i p(s, a, i) = 1\n",
        "        assert self.p.shape == (numStates, numActions, numStates), \\\n",
        "        \"The shape of the transition probabiliy matrix is incorrect!\"\n",
        "#         assert self.r.shape == (numStates, numActions), \\\n",
        "#         \"The shape of the reward matrix is incorrect!\"\n",
        "        for state in range(numStates):\n",
        "            for action in range(numActions):\n",
        "                probSum = 0\n",
        "                for nextState in range(numStates):\n",
        "                    probSum += p[state][action][nextState]\n",
        "                assert probSum == 1, \\\n",
        "                \"The sum of probabilities p({}, {}, )\".format(state, action) + \\\n",
        "                \" over all possible next staes not euqla to 1\"\n",
        "        \n",
        "        self.numStates = numStates\n",
        "        self.numActions = numActions\n",
        "        # The user can customize the prob distribution of states\n",
        "        # Otherwise, it is set to be uniform by default.\n",
        "        if stateDist:\n",
        "            self.stateDist = stateDist\n",
        "        else:\n",
        "            self.stateDist = np.ones(self.numStates) / self.numStates\n",
        "    \n",
        "    def set_state(self, state=None):\n",
        "        if state is None:\n",
        "            self.state = np.random.choice(self.numStates, p=self.stateDist)\n",
        "        else:\n",
        "            self.state = state\n",
        "        return self.state\n",
        "\n",
        "    def transit(self, action):\n",
        "        nextState = np.random.choice(self.numStates, p=self.p[self.state, action])\n",
        "        cost = self.c[self.state, action]\n",
        "        self.state = nextState\n",
        "        return nextState, cost"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO-9YaCFDIoZ"
      },
      "source": [
        "def extended_lp(env, pHat, p_conf, dsa, cost_sa, alpha, mu, timesteps):\n",
        "    ''', \n",
        "    Reference:\n",
        "        Figure 2 of the paper - Jaksch, Thomas, Ronald Ortner, and Peter Auer. \n",
        "        \"Near-optimal Regret Bounds for Reinforcement Learning.\" \n",
        "        Journal of Machine Learning Research 11, no. 4 (2010).\n",
        "    Args:\n",
        "        PsaHat: vector, MLE estimate for p(.|s, a)\n",
        "        PsaConfidenceBound: float, the confidence bound of PsaHat\n",
        "        statesSortByU: vector, states sorted by the descending order of u value\n",
        "        tolerance: float, the tolerance of the while loop in the inner maximization\n",
        "    Return:\n",
        "        Psa: vector, the optimistic transition probabilities p(.|s, a).\n",
        "        \n",
        "      zsa = extended_lp(env,pHat, pConfidenceBound, constraint_cost, cost, alpha,numStates, numActions, mu, timesteps, tolerance)\n",
        "\n",
        "    '''\n",
        "    \n",
        "    numStates, numActions = env.numStates, env.numActions\n",
        "\n",
        "    # Cost matrix for H timesteps\n",
        "    cvec_Mat = np.zeros((timesteps, numStates**2 * env.numActions))\n",
        "    \n",
        "    # p(s'|s,a) + \\beta(s,a,s')\n",
        "    qMat = pHat + p_conf\n",
        "    \n",
        "    # p(s'|s,a) - \\beta(s,a,s')\n",
        "    q_primeMat = pHat - p_conf\n",
        "\n",
        "    # Inquality constraint matrix corresponding to constraint 1\n",
        "    dvec_Mat = np.zeros((alpha.size, timesteps*numStates**2 * env.numActions))\n",
        "\n",
        "    for constr_id in range(alpha.size):\n",
        "      dvec_hsqa = np.zeros((timesteps, numStates**2 * env.numActions))\n",
        "      for i in range(timesteps):\n",
        "          # constraintMat is of dimension SxSA for each timestep          \n",
        "          constraintMat = np.tile(dsa[constr_id,:,:].reshape(1, dsa.shape[1]*dsa.shape[2]), (numStates, 1))          \n",
        "          dvec_hsqa[i, :] = np.reshape(constraintMat, numStates**2 * numActions, order='F')\n",
        "      # Update each row of constraint matrix with the vector of dimendion HS2A\n",
        "      dvec_Mat[constr_id, :] = np.reshape(dvec_hsqa.ravel(), (1, dvec_hsqa.shape[0]*dvec_hsqa.shape[1]))\n",
        "\n",
        "    # Inequality constraint matrix corresponding to constraint 2\n",
        "    Ah_eq = np.zeros((timesteps-1, timesteps * numStates**2 * env.numActions))\n",
        "\n",
        "    id = numStates**2 * numActions\n",
        "    \n",
        "    if timesteps == 1:\n",
        "      a =1\n",
        "    else:\n",
        "        for h in range(0, timesteps-1):                    \n",
        "            Ah_eq[h, h*id: (h+1)*id] = -1\n",
        "            Ah_eq[h, (h+1)*id: (h+2)*id] = 1\n",
        "\n",
        "    # Equality constraint matrix corresponding to constraint 3\n",
        "    A_eq = np.zeros((numStates, timesteps * numStates**2 * env.numActions))\n",
        "    for s in range(env.numStates):\n",
        "        A_eq[s, s*(numStates*numActions):(s+1)*(numStates*numActions)] = 1\n",
        "    \n",
        "    # Inequality constraint matrix corresponding to constraint 5\n",
        "    A_ubound = np.zeros((timesteps * numStates**2 * numActions, timesteps * numStates**2 * numActions))\n",
        "    \n",
        "    # Inequality constraint matrix corresponding to constraint 6\n",
        "    A_lbound = np.zeros((timesteps * numStates**2 * numActions, timesteps * numStates**2 * numActions))\n",
        "            \n",
        "        \n",
        "    \n",
        "    counter = 0\n",
        "    for i in range(timesteps):\n",
        "        for j in range(numStates):\n",
        "            for k in range(numActions):\n",
        "                for l in range(numStates):               \n",
        "                    \n",
        "                    for s_prime in range(-l, -l + numStates):\n",
        "                        \n",
        "                        A_ubound[counter, counter + s_prime] = -qMat[i,j,k,l + s_prime]\n",
        "                        A_lbound[counter, counter + s_prime] = q_primeMat[i,j,k,l + s_prime]\n",
        "                    # Update diagonal elements\n",
        "                    A_ubound[counter, counter] = 1-qMat[i,j,k,l]\n",
        "                    A_lbound[counter, counter] = -1+q_primeMat[i,j,k,l]\n",
        "                    \n",
        "                    counter += 1\n",
        "    \n",
        "\n",
        "    A_lbound = A_lbound.T\n",
        "    A_ubound = A_ubound.T\n",
        "\n",
        "    for h in range(timesteps):\n",
        "      temp = np.tile(cost_sa.reshape(1, cost_sa.shape[0]*cost_sa.shape[1]), (numStates, 1))\n",
        "      cvec_Mat[h, :] = np.reshape(temp, numStates**2 * env.numActions, order='F')  \n",
        "\n",
        "    # Coefficients of linear objective function    \n",
        "    # cost_vec = cvec_Mat.ravel()    \n",
        "    \n",
        "    # Define optimization variables    \n",
        "    c_final = cvec_Mat.ravel()\n",
        "\n",
        "    # Equality constraint vector corresponding to constraint 3\n",
        "    b_eq = np.concatenate((mu, np.zeros(timesteps-1)),axis=0)\n",
        "    \n",
        "\n",
        "    temp = np.reshape(np.asarray(alpha), (alpha.size,1))\n",
        "    # Inquality constraint vector corresponding to constraint 1\n",
        "    b_ub = np.concatenate((temp, np.zeros((2*numStates**2 * numActions*timesteps, 1))), axis=0)\n",
        "    \n",
        "    A_eq_final = np.concatenate((A_eq, Ah_eq), axis=0)\n",
        "    A_ub_final = np.concatenate((dvec_Mat, A_ubound, A_lbound), axis=0)\n",
        "    \n",
        "    res = linprog(c_final, A_eq=A_eq_final, b_eq=b_eq, A_ub=A_ub_final, b_ub=b_ub)\n",
        "    \n",
        "    return res.x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zllXufEDIoc"
      },
      "source": [
        "def extended_value_iteration(env, timesteps, stateActionCounts, pHat, pConfidenceBound, cost, constraint_cost, alpha):\n",
        "    '''\n",
        "    Reference:\n",
        "        3.1.2 EXTENDED VALUE ITERATION of the paper - \n",
        "        Jaksch, Thomas, Ronald Ortner, and Peter Auer. \n",
        "        \"Near-optimal Regret Bounds for Reinforcement Learning.\" \n",
        "        Journal of Machine Learning Research 11, no. 4 (2010).\n",
        "    Args:\n",
        "        numStates: integer, number of states of the MDP\n",
        "        numActions: integer, number of actions of the MDP\n",
        "        pHat: matrix, MLE estimates of state-by-action-by-state transition probability matrix\n",
        "        pConfidenceBound: float, optimistic confidence bounds for pHat\n",
        "        rHat: matrix, MLE estimates of state-by-action rewards matrix\n",
        "        rConfidenceBound: float, optimistic confidence bounds for rHat\n",
        "        epsilon: float, the epsilon of Page 30 of the referenced slides below\n",
        "            https://www.math.univ-toulouse.fr/~agarivie/sites/default/files/saopaulo1010RL.pdf\n",
        "        tolerance: float, the tolerance of the while loop in the inner maximization\n",
        "    Return:\n",
        "        (pTilde, rTilde) - params that can determine the optimistic MDP Mk\n",
        "            pTilde: matrix, state-by-action-by-state transition probability matrix of Mk\n",
        "            rTilde: matrix, state-by-action reward matrix of Mk\n",
        "        piTilde: vector, optimal policy of Mk, piTilde[i] is the optimal action at state i\n",
        "    '''\n",
        "    \n",
        "    # Initiate state values u_0 in Eq (5) of the paper\n",
        "    # stateUDiff is the diference of values u u_{i}-u_{i-1}\n",
        "    # between two iterations\n",
        "    \n",
        "    \n",
        "    zHat = np.random.uniform(0, 1, (numStates, numActions, numStates))\n",
        "    for i in range(numActions):\n",
        "        zHat[:, i, :] =(zHat[:, i, :].T/np.sum(zHat[:, i, :], axis = 1)).T\n",
        "    \n",
        "    \n",
        "    # Initialized the state-action-state transition probability matrix pTilde\n",
        "    # and the optimal policy piTilde for the optimistic MDP\n",
        "    pTilde = np.zeros((timesteps,numStates, numActions, numStates))\n",
        "    piTilde = np.zeros((timesteps, numStates), dtype='int')\n",
        "\n",
        "    pi_vec = np.zeros((timesteps, numStates, numActions))\n",
        "    \n",
        "    #Prior distribution\n",
        "    mu = np.ones(numStates) / numStates\n",
        "    \n",
        "    zsa = extended_lp(env, pHat, pConfidenceBound, constraint_cost, cost, alpha, mu, timesteps)\n",
        "\n",
        "    '''\n",
        "    Calculate p(s'|s,a) and \\pi(a|s)\n",
        "    TBD: Vectorize code\n",
        "    '''\n",
        "    zMat = zsa.reshape(timesteps,numStates, numActions, numStates)\n",
        "    for h in range(timesteps):\n",
        "      for s in range(numStates):\n",
        "        pi_den = np.sum(zMat[h,s,:,:])\n",
        "        for a in range(numActions):\n",
        "          sum = np.sum(zMat[h,s,a,:])\n",
        "          pi_vec[h, s, a] = sum / pi_den\n",
        "          for y in range(numStates):\n",
        "            pTilde[h,s,a,y] = zMat[h,s,a,y] / sum\n",
        "\n",
        "    for h in range(timesteps):\n",
        "      for s in range(numStates):\n",
        "        piTilde[h,s] = np.argmax(pi_vec[h,s,:])\n",
        "\n",
        "    return piTilde, pTilde"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd8kQo-DDIof"
      },
      "source": [
        "def cmdp_algo(env, delta, timesteps, episode_length=5, initialState=None):\n",
        "    \n",
        "    numStates, numActions = env.numStates, env.numActions\n",
        "    t = 1\n",
        "    \n",
        "    # Initial state\n",
        "    state = env.set_state(initialState)\n",
        "    \n",
        "    # Model estimates\n",
        "    stateActionCounts = np.zeros((numStates, numActions))\n",
        "    stateActionCosts = np.zeros((numStates, numActions))\n",
        "    stateActionStateCounts = np.zeros((numStates, numActions, numStates))\n",
        "    \n",
        "    pHat = np.ones((numStates, numActions, numStates)) / numStates\n",
        "    pHat = np.repeat(pHat[np.newaxis, :, :, :], timesteps, axis=0) #Dimension HSAS'\n",
        "   \n",
        "    \n",
        "    for k in range(episode_length):\n",
        "        # Set the start time of episode k, Tk :=t.\n",
        "        Tk = t\n",
        "        \n",
        "        # For all (s,a) in S Ã—A initialize the state-action counts for episode k, Vk(s,a) := 0##\n",
        "        Vk = np.zeros((numStates, numActions))\n",
        "        \n",
        "        #pConfidenceBound = np.sqrt(np.multiply(pHat, 1-pHat) / np.clip(stateActionCounts, 1.0, None)) + (1 / np.clip(stateActionCounts, 1.0, None))\n",
        "        # Ensuring dimension of pConfidenceBound is HxSxAxS\n",
        "        temp = np.clip(stateActionCounts, 1.0, None)\n",
        "        pConfidenceBound = np.sqrt(np.multiply(pHat, 1-pHat) / np.repeat(temp[:, :, np.newaxis], pHat.shape[3], axis=pHat.shape[3])) + np.repeat((1 / temp)[:, :, np.newaxis], pHat.shape[3], axis=pHat.shape[3])\n",
        "\n",
        "        \n",
        "        PIk, Mk = extended_value_iteration(env, timesteps, stateActionCounts, pHat, pConfidenceBound, env.c, env.d, env.alpha)\n",
        "        \n",
        "        # Execute the policy\n",
        "        for h in range(timesteps):\n",
        "          action = PIk[h, state]\n",
        "          nextState, cost = env.transit(action)\n",
        "          yield (t, state, action, nextState, cost)   \n",
        "          Vk[state, action] += 1\n",
        "          stateActionCosts[state, action] += cost\n",
        "          stateActionStateCounts[state, action, nextState] += 1  \n",
        "          t += 1\n",
        "          state = nextState  \n",
        "          action = PIk[state]\n",
        "            \n",
        "          \n",
        "        pHat = Mk"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h7E_ZkyVc8t",
        "outputId": "3d7a8e2a-14cd-4007-f606-dd4f649f2d39"
      },
      "source": [
        "theta = 0.1\n",
        "numStates = 2\n",
        "numActions = 2\n",
        "p = np.array([\n",
        "    [[1, 0], [.3, .7]],\n",
        "    [[0.1, 0.9], [.2, .8]]\n",
        "])\n",
        "print('Transition matrix shape',p.shape)\n",
        "\n",
        "c = np.reshape(np.array([[.5, .9], [.3, .6]]), (numStates*numActions,1))\n",
        "d = np.reshape(np.array([\n",
        "                         [[.4, .8], [.1, .3]],\n",
        "                         [[.25, .7], [.15, .8]]\n",
        "                         ]), (2,numStates*numActions,1))\n",
        "print(d.shape)\n",
        "alpha = np.array([0.5, .2])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transition matrix shape (2, 2, 2)\n",
            "(2, 4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKtTFycbDIof",
        "scrolled": false,
        "outputId": "931b3289-c33b-480b-beb4-8b4bf087df08"
      },
      "source": [
        "experiments = []\n",
        "numSteps = 400\n",
        "numRepeats = 20\n",
        "timesteps = 1\n",
        "NUM_EPS = 60\n",
        "for i in range(numRepeats):\n",
        "    env = MDP(numStates, numActions, p, c, d, alpha)\n",
        "    transitions = cmdp_algo(env, delta=0.1, timesteps=1, episode_length=NUM_EPS, initialState=0)\n",
        "    \n",
        "    curtExp = []\n",
        "    for j in range(NUM_EPS):\n",
        "        (t, state, action, nxtState, cost) = transitions.__next__()\n",
        "        curtExp.append((t, state, action, nxtState, cost))\n",
        "    experiments.append(curtExp)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHtf2AvMDIog"
      },
      "source": [
        "cumRegrets = np.zeros([numRepeats, numSteps])\n",
        "perStepRegrets = np.zeros([numRepeats, numSteps])\n",
        "optimalCost = 0.0\n",
        "\n",
        "for i in range(numRepeats):\n",
        "    cumRegret = 0.0\n",
        "    for j in range(NUM_EPS):\n",
        "        (t, state, action, nxtState, cost) = experiments[i][j]\n",
        "        cumRegret += cost - optimalCost\n",
        "        cumRegrets[i, j] = cumRegret\n",
        "        perStepRegrets[i, j] = cumRegret / t"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN1n-mlzDIog",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "39e483df-006c-4bbb-af73-b89dd9b423c7"
      },
      "source": [
        "plt.plot(cumRegrets.mean(axis=0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f91a55bf650>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSFMcnsbDIoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "70f2695d-af70-4f1b-90d3-5c6823aa676a"
      },
      "source": [
        "plt.plot(perStepRegrets.mean(axis=0))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f91a50aed10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQ4ElEQVR4nO3dfaxkdX3H8ffHXUCjVFE2lrKsi5SkWR+KeEWbGjQoEdCyJmqyJG2wtVlb3Wi1TV1iQyxNk4qpD62kurXUh1YBbZts7RpqlbY2DbCLIrKQheuKhY2FVfApjeDqt3/MWZm93oe5d+fM7P35fiWTPQ+/Oed7fzP3s3PP+c05qSokSavfY6ZdgCRpPAx0SWqEgS5JjTDQJakRBrokNWLttHZ88skn18aNG6e1e0lalW655ZZvVtW6+dZNLdA3btzInj17prV7SVqVknx9oXUecpGkRhjoktQIA12SGmGgS1IjDHRJasRIgZ7kgiT7kswm2T7P+tcmOZjk1u7x2+MvVZK0mCWHLSZZA1wFnA/cB+xOsrOq7pjT9Nqq2tZDjZKkEYwyDv0cYLaq9gMkuQbYDMwN9InYfc+DfOGug9PY9VE7f9PP86z1T5x2GZIaNUqgnwrcOzR/H/D8edq9Ksm5wF3AW6rq3rkNkmwFtgJs2LBh+dUCX/z6Q/zlDbMreu40VcG++7/HB39jZtqlSGrUuL4p+s/AJ6rq4SSvBz4CnDe3UVXtAHYAzMzMrOjOGq9/0Rm8/kVnHE2tU3HR+77Aj3487SoktWyUk6IHgNOG5td3y36iqr5VVQ93sx8Cnjue8iRJoxol0HcDZyY5PcnxwBZg53CDJKcMzV4M3Dm+EiVJo1jykEtVHUqyDbgeWANcXVV7k1wB7KmqncCbklwMHAIeBF7bY82SpHmMdAy9qnYBu+Ysu3xo+jLgsvGWJklaDr8pKkmNMNAnakUDeyRpJAb6hCTTrkBS6wx0SWqEgS5JjTDQJakRBrokNcJAn6BykIukHhnoktQIA31CHLYoqW8GuiQ1wkCXpEYY6JLUCANdkhphoE+QoxYl9clAl6RGGOgTEhy3KKlfBrokNcJAl6RGGOiS1AgDXZIaYaBPUHm5RUk9MtAlqREG+oR4tUVJfTPQJakRBrokNcJAl6RGGOiS1AgDfYIctCipTwb6hDjIRVLfDHRJasRIgZ7kgiT7kswm2b5Iu1clqSQz4ytRkjSKJQM9yRrgKuBCYBNwSZJN87Q7EXgzcNO4i5QkLW2UT+jnALNVtb+qHgGuATbP0+5PgHcCPxhjfZKkEY0S6KcC9w7N39ct+4kkZwOnVdW/LLahJFuT7Emy5+DBg8sudrXz2lyS+nTUJ0WTPAZ4N/D7S7Wtqh1VNVNVM+vWrTvaXUuShowS6AeA04bm13fLDjsReCbw70nuAV4A7PTE6BxenUtSz0YJ9N3AmUlOT3I8sAXYeXhlVX2nqk6uqo1VtRG4Ebi4qvb0UrEkaV5LBnpVHQK2AdcDdwLXVdXeJFckubjvAiVJo1k7SqOq2gXsmrPs8gXavvjoy5IkLZffFJWkRhjoE+SoRUl9MtAlqREG+oQ4aFFS3wx0SWqEgS5JjTDQJakRBrokNcJAn6DycouSemSgS1IjDPQJ8WKLkvpmoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOgT4qhFSX0z0CWpEQa6JDXCQJekRhjoktQIA32CvNiipD4Z6BMSr84lqWcGuiQ1wkCXpEYY6JLUCANdkhphoE9Q4TAXSf0x0CWpESMFepILkuxLMptk+zzrfyfJV5LcmuS/kmwaf6mrm4MWJfVtyUBPsga4CrgQ2ARcMk9gf7yqnlVVZwFXAu8ee6WSpEWN8gn9HGC2qvZX1SPANcDm4QZV9d2h2ceDB4sladLWjtDmVODeofn7gOfPbZTkjcBbgeOB8+bbUJKtwFaADRs2LLdWSdIixnZStKquqqozgLcBf7RAmx1VNVNVM+vWrRvXriVJjBboB4DThubXd8sWcg3wyqMpqlVenEtSn0YJ9N3AmUlOT3I8sAXYOdwgyZlDsy8H7h5fiZKkUSx5DL2qDiXZBlwPrAGurqq9Sa4A9lTVTmBbkpcCPwQeAi7ts+jVyIstSurbKCdFqapdwK45yy4fmn7zmOuSJC2T3xSVpEYY6JLUCANdkhphoE+QwxYl9clAl6RGGOgTEq+3KKlnBrokNcJAl6RGGOiS1AgDXZIaYaBPkDeJltQnA12SGmGgT4qjFiX1zECXpEYY6JLUCANdkhphoEtSIwz0CfJqi5L6ZKBPiINcJPXNQJekRhjoktQIA12SGmGgS1IjDPQJcpCLpD4Z6JLUCAN9QuK4RUk9M9AlqREGuiQ1wkCXpEYY6JLUCAN9khy3KKlHIwV6kguS7Esym2T7POvfmuSOJLcl+VySp42/VEnSYpYM9CRrgKuAC4FNwCVJNs1p9iVgpqqeDXwKuHLcha528XqLkno2yif0c4DZqtpfVY8A1wCbhxtU1Q1V9X/d7I3A+vGWKUlayiiBfipw79D8fd2yhbwO+Mx8K5JsTbInyZ6DBw+OXqUkaUljPSma5NeBGeBd862vqh1VNVNVM+vWrRvnriXpZ97aEdocAE4bml/fLTtCkpcCbwdeVFUPj6c8SdKoRvmEvhs4M8npSY4HtgA7hxskeQ7wQeDiqnpg/GW2oRy3KKlHSwZ6VR0CtgHXA3cC11XV3iRXJLm4a/Yu4AnAJ5PcmmTnApuTJPVklEMuVNUuYNecZZcPTb90zHU1x6stSuqb3xSVpEYY6JLUCANdkhphoEtSIwz0CSpHLUrqkYEuSY0w0CfEYYuS+magS1IjDHRJaoSBLkmNMNAlqREG+gQ5alFSnwz0CfGeopL6ZqBLUiMMdElqhIEuSY0w0CWpEQb6BJVX55LUIwNdkhphoE+IF+eS1DcDXZIaYaBLUiMMdElqhIEuSY0w0CfIQYuS+mSgS1IjDHRJaoSBLkmNMNAlqREGuiQ1YqRAT3JBkn1JZpNsn2f9uUm+mORQklePv0xJ0lKWDPQka4CrgAuBTcAlSTbNafY/wGuBj4+7wJZ4sUVJfVo7QptzgNmq2g+Q5BpgM3DH4QZVdU+37sc91ChJGsEoh1xOBe4dmr+vW7ZsSbYm2ZNkz8GDB1eyiVUrXm5RUs8melK0qnZU1UxVzaxbt26Su5ak5o0S6AeA04bm13fLJEnHkFECfTdwZpLTkxwPbAF29luWJGm5lgz0qjoEbAOuB+4ErquqvUmuSHIxQJLnJbkPeA3wwSR7+yxakvTTRhnlQlXtAnbNWXb50PRuBoditAhHLUrqk98UnRDHuEjqm4EuSY0w0CWpEQa6JDXCQJekRhjoktQIA32SvNyipB4Z6BPitbkk9c1Al6RGGOiS1AgDXZIaYaBLUiMM9AlyjIukPhnoktQIA31CHLUoqW8GuiQ1wkCXpEYY6JLUCANdkhphoE+Q1+aS1CcDXZIaYaBPSLzcoqSeGeiS1AgDXZIaYaBLUiMMdElqhIE+QeX1FiX1yECXpEYY6BPioEVJfTPQJakRBrokNWKkQE9yQZJ9SWaTbJ9n/QlJru3W35Rk47gLlSQtbslAT7IGuAq4ENgEXJJk05xmrwMeqqpfBN4DvHPchUqSFrd2hDbnALNVtR8gyTXAZuCOoTabgXd0058C3p8kVV5fcNhd93+f89/9H9MuQ9KUveklZ/Jrv/wLY9/uKIF+KnDv0Px9wPMXalNVh5J8B3gK8M3hRkm2AlsBNmzYsMKSV6ct52zghOM8ZSEJnvi443rZ7iiBPjZVtQPYATAzM/Mz9en9/E1P5fxNT512GZIaNspHxgPAaUPz67tl87ZJshZ4IvCtcRQoSRrNKIG+GzgzyelJjge2ADvntNkJXNpNvxr4vMfPJWmyljzk0h0T3wZcD6wBrq6qvUmuAPZU1U7gb4CPJZkFHmQQ+pKkCRrpGHpV7QJ2zVl2+dD0D4DXjLc0SdJyOOxCkhphoEtSIwx0SWqEgS5Jjci0RhcmOQh8fYVPP5k530I9RljX8ljX8h2rtVnX8hxNXU+rqnXzrZhaoB+NJHuqambadcxlXctjXct3rNZmXcvTV10ecpGkRhjoktSI1RroO6ZdwAKsa3msa/mO1dqsa3l6qWtVHkOXJP201foJXZI0h4EuSY1YdYG+1A2re9rnPUm+kuTWJHu6ZU9O8tkkd3f/ntQtT5K/6Oq7LcnZQ9u5tGt/d5JLF9rfInVcneSBJLcPLRtbHUme2/2cs91zcxR1vSPJga7Pbk1y0dC6y7p97EvysqHl87623aWbb+qWX9tdxnmUuk5LckOSO5LsTfLmY6HPFqlrqn2W5LFJbk7y5a6uP15sW1nk5vDLrXeFdX04ydeG+uusbvkk3/trknwpyaePhb6iqlbNg8Hle78KPB04HvgysGkC+70HOHnOsiuB7d30duCd3fRFwGeAAC8AbuqWPxnY3/17Ujd90jLrOBc4G7i9jzqAm7u26Z574VHU9Q7gD+Zpu6l73U4ATu9ezzWLvbbAdcCWbvoDwO+OWNcpwNnd9InAXd3+p9pni9Q11T7rfoYndNPHATd1P9u82wLeAHygm94CXLvSeldY14eBV8/TfpLv/bcCHwc+vVi/T6qvVtsn9J/csLqqHgEO37B6GjYDH+mmPwK8cmj5R2vgRuBJSU4BXgZ8tqoerKqHgM8CFyxnh1X1nwyuNz/2Orp1P1dVN9bgnfbRoW2tpK6FbAauqaqHq+prwCyD13Xe17b7pHQeg5uPz/0Zl6rrG1X1xW76e8CdDO5/O9U+W6SuhUykz7qf+/vd7HHdoxbZ1nA/fgp4SbfvZdV7FHUtZCKvY5L1wMuBD3Xzi/X7RPpqtQX6fDesXuwXYVwK+Nckt2Rwo2uAp1bVN7rp/wUO3zB0oRr7qn1cdZzaTY+zvm3dn7xXpzussYK6ngJ8u6oOHU1d3Z+4z2Hw6e6Y6bM5dcGU+6w7hHAr8ACDwPvqIts64ubwwOGbw4/9d2BuXVV1uL/+tOuv9yQ5YW5dI+5/pa/je4E/BH7czS/W7xPpq9UW6NPywqo6G7gQeGOSc4dXdv+rT33857FSR+evgDOAs4BvAH8+rUKSPAH4B+D3quq7w+um2Wfz1DX1PquqH1XVWQzuHXwO8EuTrmE+c+tK8kzgMgb1PY/BYZS3TaqeJK8AHqiqWya1z1GstkAf5YbVY1dVB7p/HwD+icEb/f7uTzW6fx9Yosa+ah9XHQe66bHUV1X3d7+EPwb+mkGfraSubzH4k3ntnOUjSXIcg9D8+6r6x27x1PtsvrqOlT7ravk2cAPwK4tsa6Gbw/f2OzBU1wXdoauqqoeBv2Xl/bWS1/FXgYuT3MPgcMh5wPuYdl8tdZD9WHowuGXefgYnDw6fKHhGz/t8PHDi0PR/Mzj2/S6OPLF2ZTf9co48IXNzPXpC5msMTsac1E0/eQX1bOTIk49jq4OfPjF00VHUdcrQ9FsYHCcEeAZHngTaz+AE0IKvLfBJjjzR9IYRawqD46HvnbN8qn22SF1T7TNgHfCkbvpxwBeAVyy0LeCNHHmi77qV1rvCuk4Z6s/3An82pff+i3n0pOh0+2q5gTLtB4Mz2HcxOLb39gns7+ldZ34Z2Ht4nwyOf30OuBv4t6E3RoCruvq+AswMbeu3GJz0mAV+cwW1fILBn+I/ZHBM7XXjrAOYAW7vnvN+um8Sr7Cuj3X7vQ3YyZFh9fZuH/sYGk2w0GvbvQY3d/V+EjhhxLpeyOBwym3Ard3jomn32SJ1TbXPgGcDX+r2fztw+WLbAh7bzc9265++0npXWNfnu/66Hfg7Hh0JM7H3fvfcF/NooE+1r/zqvyQ1YrUdQ5ckLcBAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY34f1XfqzTpRxtVAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NenyZM-ejva5"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}