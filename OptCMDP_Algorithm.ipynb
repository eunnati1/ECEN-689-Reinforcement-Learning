{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "OptCMDP Algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunnati1/ECEN-689-Reinforcement-Learning/blob/main/OptCMDP_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7QO_5_9DIoI"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import math\n",
        "import statistics as stat\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "from scipy.optimize import linprog"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNUXSJfwkORy"
      },
      "source": [
        "class MDP:\n",
        "    def __init__(self, numStates, numActions, p, c, d, alpha, stateDist=None):\n",
        "        \"\"\"temp[]\n",
        "        Reference:\n",
        "        Args:\n",
        "            numStates: integer, number of states of the MDP\n",
        "            numActions: integer, number of actions of the MDP\n",
        "            p: matrix, numStates-by-numActions-by-numStates probability \n",
        "                transition matrix of the MDP\n",
        "            r: matrix, numStates-by-numActions reward matrixof the MDP\n",
        "            stateDist: vector, 1-by-numState, the initial probability distribution\n",
        "                 over the states of the MDP, default is uniform distribution\n",
        "        Return:\n",
        "            A MDP object with the specified numStates, numActions, p, r\n",
        "            and stateDist\n",
        "        \"\"\"\n",
        "        \n",
        "        self.p = np.asarray(p)\n",
        "        self.c = c\n",
        "        self.d = d\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # The size of p is numStates by numActions by numStates\n",
        "        # p(s, a, s') denotes the probability to transit to state\n",
        "        # s' after taking action a at state s.\n",
        "        # As such, \\sum_i p(s, a, i) = 1\n",
        "        assert self.p.shape == (numStates, numActions, numStates), \\\n",
        "        \"The shape of the transition probabiliy matrix is incorrect!\"\n",
        "#         assert self.r.shape == (numStates, numActions), \\\n",
        "#         \"The shape of the reward matrix is incorrect!\"\n",
        "        for state in range(numStates):\n",
        "            for action in range(numActions):\n",
        "                probSum = 0\n",
        "                for nextState in range(numStates):\n",
        "                    probSum += p[state][action][nextState]\n",
        "                assert probSum == 1, \\\n",
        "                \"The sum of probabilities p({}, {}, )\".format(state, action) + \\\n",
        "                \" over all possible next staes not euqla to 1\"\n",
        "        \n",
        "        self.numStates = numStates\n",
        "        self.numActions = numActions\n",
        "        # The user can customize the prob distribution of states\n",
        "        # Otherwise, it is set to be uniform by default.\n",
        "        if stateDist:\n",
        "            self.stateDist = stateDist\n",
        "        else:\n",
        "            self.stateDist = np.ones(self.numStates) / self.numStates\n",
        "    \n",
        "    def set_state(self, state=None):\n",
        "        if state is None:\n",
        "            self.state = np.random.choice(self.numStates, p=self.stateDist)\n",
        "        else:\n",
        "            self.state = state\n",
        "        return self.state\n",
        "\n",
        "    def transit(self, action):\n",
        "        nextState = np.random.choice(self.numStates, p=self.p[self.state, action])\n",
        "        cost = self.c[self.state, action]\n",
        "        self.state = nextState\n",
        "        return nextState, cost"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO-9YaCFDIoZ"
      },
      "source": [
        "def extended_lp(env, pHat, p_conf, dsa, cost_sa, alpha, mu, timesteps):\n",
        "    ''', \n",
        "    Reference:\n",
        "        Figure 2 of the paper - Jaksch, Thomas, Ronald Ortner, and Peter Auer. \n",
        "        \"Near-optimal Regret Bounds for Reinforcement Learning.\" \n",
        "        Journal of Machine Learning Research 11, no. 4 (2010).\n",
        "    Args:\n",
        "        PsaHat: vector, MLE estimate for p(.|s, a)\n",
        "        PsaConfidenceBound: float, the confidence bound of PsaHat\n",
        "        statesSortByU: vector, states sorted by the descending order of u value\n",
        "        tolerance: float, the tolerance of the while loop in the inner maximization\n",
        "    Return:\n",
        "        Psa: vector, the optimistic transition probabilities p(.|s, a).\n",
        "        \n",
        "      zsa = extended_lp(env,pHat, pConfidenceBound, constraint_cost, cost, alpha,numStates, numActions, mu, timesteps, tolerance)\n",
        "\n",
        "    '''\n",
        "    \n",
        "    numStates, numActions = env.numStates, env.numActions\n",
        "\n",
        "    # Cost matrix for H timesteps\n",
        "    cvec_Mat = np.zeros((timesteps, numStates**2 * env.numActions))\n",
        "    \n",
        "    # p(s'|s,a) + \\beta(s,a,s')\n",
        "    qMat = pHat + p_conf\n",
        "    \n",
        "    # p(s'|s,a) - \\beta(s,a,s')\n",
        "    q_primeMat = pHat - p_conf\n",
        "\n",
        "    # Inquality constraint matrix corresponding to constraint 1\n",
        "    dvec_Mat = np.zeros((alpha.size, timesteps*numStates**2 * env.numActions))\n",
        "\n",
        "    for constr_id in range(alpha.size):\n",
        "      dvec_hsqa = np.zeros((timesteps, numStates**2 * env.numActions))\n",
        "      for i in range(timesteps):\n",
        "          # constraintMat is of dimension SxSA for each timestep          \n",
        "          constraintMat = np.tile(dsa[constr_id,:,:].reshape(1, dsa.shape[1]*dsa.shape[2]), (numStates, 1))          \n",
        "          dvec_hsqa[i, :] = np.reshape(constraintMat, numStates**2 * numActions, order='F')\n",
        "      # Update each row of constraint matrix with the vector of dimendion HS2A\n",
        "      dvec_Mat[constr_id, :] = np.reshape(dvec_hsqa.ravel(), (1, dvec_hsqa.shape[0]*dvec_hsqa.shape[1]))\n",
        "\n",
        "    # Inequality constraint matrix corresponding to constraint 2\n",
        "    Ah_eq = np.zeros((timesteps-1, timesteps * numStates**2 * env.numActions))\n",
        "\n",
        "    id = numStates**2 * numActions\n",
        "    \n",
        "    if timesteps == 1:\n",
        "      a =1\n",
        "    else:\n",
        "        for h in range(0, timesteps-1):                    \n",
        "            Ah_eq[h, h*id: (h+1)*id] = -1\n",
        "            Ah_eq[h, (h+1)*id: (h+2)*id] = 1\n",
        "\n",
        "    # Equality constraint matrix corresponding to constraint 3\n",
        "    A_eq = np.zeros((numStates, timesteps * numStates**2 * env.numActions))\n",
        "    for s in range(env.numStates):\n",
        "        A_eq[s, s*(numStates*numActions):(s+1)*(numStates*numActions)] = 1\n",
        "    \n",
        "    # Inequality constraint matrix corresponding to constraint 5\n",
        "    A_ubound = np.zeros((timesteps * numStates**2 * numActions, timesteps * numStates**2 * numActions))\n",
        "    \n",
        "    # Inequality constraint matrix corresponding to constraint 6\n",
        "    A_lbound = np.zeros((timesteps * numStates**2 * numActions, timesteps * numStates**2 * numActions))\n",
        "            \n",
        "        \n",
        "    \n",
        "    counter = 0\n",
        "    for i in range(timesteps):\n",
        "        for j in range(numStates):\n",
        "            for k in range(numActions):\n",
        "                for l in range(numStates):               \n",
        "                    \n",
        "                    for s_prime in range(-l, -l + numStates):\n",
        "                        \n",
        "                        A_ubound[counter, counter + s_prime] = -qMat[i,j,k,l + s_prime]\n",
        "                        A_lbound[counter, counter + s_prime] = q_primeMat[i,j,k,l + s_prime]\n",
        "                    # Update diagonal elements\n",
        "                    A_ubound[counter, counter] = 1-qMat[i,j,k,l]\n",
        "                    A_lbound[counter, counter] = -1+q_primeMat[i,j,k,l]\n",
        "                    \n",
        "                    counter += 1\n",
        "    \n",
        "\n",
        "    A_lbound = A_lbound.T\n",
        "    A_ubound = A_ubound.T\n",
        "\n",
        "    for h in range(timesteps):\n",
        "      temp = np.tile(cost_sa.reshape(1, cost_sa.shape[0]*cost_sa.shape[1]), (numStates, 1))\n",
        "      cvec_Mat[h, :] = np.reshape(temp, numStates**2 * env.numActions, order='F')  \n",
        "\n",
        "    # Coefficients of linear objective function    \n",
        "    # cost_vec = cvec_Mat.ravel()    \n",
        "    \n",
        "    # Define optimization variables    \n",
        "    c_final = cvec_Mat.ravel()\n",
        "\n",
        "    # Equality constraint vector corresponding to constraint 3\n",
        "    b_eq = np.concatenate((mu, np.zeros(timesteps-1)),axis=0)\n",
        "    \n",
        "\n",
        "    temp = np.reshape(np.asarray(alpha), (alpha.size,1))\n",
        "    # Inquality constraint vector corresponding to constraint 1\n",
        "    b_ub = np.concatenate((temp, np.zeros((2*numStates**2 * numActions*timesteps, 1))), axis=0)\n",
        "    \n",
        "    A_eq_final = np.concatenate((A_eq, Ah_eq), axis=0)\n",
        "    A_ub_final = np.concatenate((dvec_Mat, A_ubound, A_lbound), axis=0)\n",
        "    \n",
        "    res = linprog(c_final, A_eq=A_eq_final, b_eq=b_eq, A_ub=A_ub_final, b_ub=b_ub)\n",
        "    \n",
        "    return res.x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zllXufEDIoc"
      },
      "source": [
        "def extended_value_iteration(env, timesteps, stateActionCounts, pHat, pConfidenceBound, cost, constraint_cost, alpha):\n",
        "    '''\n",
        "    Reference:\n",
        "        3.1.2 EXTENDED VALUE ITERATION of the paper - \n",
        "        Jaksch, Thomas, Ronald Ortner, and Peter Auer. \n",
        "        \"Near-optimal Regret Bounds for Reinforcement Learning.\" \n",
        "        Journal of Machine Learning Research 11, no. 4 (2010).\n",
        "    Args:\n",
        "        numStates: integer, number of states of the MDP\n",
        "        numActions: integer, number of actions of the MDP\n",
        "        pHat: matrix, MLE estimates of state-by-action-by-state transition probability matrix\n",
        "        pConfidenceBound: float, optimistic confidence bounds for pHat\n",
        "        rHat: matrix, MLE estimates of state-by-action rewards matrix\n",
        "        rConfidenceBound: float, optimistic confidence bounds for rHat\n",
        "        epsilon: float, the epsilon of Page 30 of the referenced slides below\n",
        "            https://www.math.univ-toulouse.fr/~agarivie/sites/default/files/saopaulo1010RL.pdf\n",
        "        tolerance: float, the tolerance of the while loop in the inner maximization\n",
        "    Return:\n",
        "        (pTilde, rTilde) - params that can determine the optimistic MDP Mk\n",
        "            pTilde: matrix, state-by-action-by-state transition probability matrix of Mk\n",
        "            rTilde: matrix, state-by-action reward matrix of Mk\n",
        "        piTilde: vector, optimal policy of Mk, piTilde[i] is the optimal action at state i\n",
        "    '''\n",
        "    \n",
        "    # Initiate state values u_0 in Eq (5) of the paper\n",
        "    # stateUDiff is the diference of values u u_{i}-u_{i-1}\n",
        "    # between two iterations\n",
        "    \n",
        "    \n",
        "    zHat = np.random.uniform(0, 1, (numStates, numActions, numStates))\n",
        "    for i in range(numActions):\n",
        "        zHat[:, i, :] =(zHat[:, i, :].T/np.sum(zHat[:, i, :], axis = 1)).T\n",
        "    \n",
        "    \n",
        "    # Initialized the state-action-state transition probability matrix pTilde\n",
        "    # and the optimal policy piTilde for the optimistic MDP\n",
        "    pTilde = np.zeros((timesteps,numStates, numActions, numStates))\n",
        "    piTilde = np.zeros((timesteps, numStates), dtype='int')\n",
        "\n",
        "    pi_vec = np.zeros((timesteps, numStates, numActions))\n",
        "    \n",
        "    #Prior distribution\n",
        "    mu = np.ones(numStates) / numStates\n",
        "    \n",
        "    zsa = extended_lp(env, pHat, pConfidenceBound, constraint_cost, cost, alpha, mu, timesteps)\n",
        "\n",
        "    '''\n",
        "    Calculate p(s'|s,a) and \\pi(a|s)\n",
        "    TBD: Vectorize code\n",
        "    '''\n",
        "    zMat = zsa.reshape(timesteps,numStates, numActions, numStates)\n",
        "    for h in range(timesteps):\n",
        "      for s in range(numStates):\n",
        "        pi_den = np.sum(zMat[h,s,:,:])\n",
        "        for a in range(numActions):\n",
        "          sum = np.sum(zMat[h,s,a,:])\n",
        "          pi_vec[h, s, a] = sum / pi_den\n",
        "          for y in range(numStates):\n",
        "            pTilde[h,s,a,y] = zMat[h,s,a,y] / sum\n",
        "\n",
        "    for h in range(timesteps):\n",
        "      for s in range(numStates):\n",
        "        piTilde[h,s] = np.argmax(pi_vec[h,s,:])\n",
        "\n",
        "    return piTilde, pTilde"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd8kQo-DDIof"
      },
      "source": [
        "def cmdp_algo(env, delta, timesteps, episode_length=5, initialState=None):\n",
        "    \n",
        "    numStates, numActions = env.numStates, env.numActions\n",
        "    t = 1\n",
        "    \n",
        "    # Initial state\n",
        "    state = env.set_state(initialState)\n",
        "    \n",
        "    # Model estimates\n",
        "    stateActionCounts = np.zeros((numStates, numActions))\n",
        "    stateActionCosts = np.zeros((numStates, numActions))\n",
        "    stateActionStateCounts = np.zeros((numStates, numActions, numStates))\n",
        "    \n",
        "    pHat = np.ones((numStates, numActions, numStates)) / numStates\n",
        "    pHat = np.repeat(pHat[np.newaxis, :, :, :], timesteps, axis=0) #Dimension HSAS'\n",
        "   \n",
        "    \n",
        "    for k in range(episode_length):\n",
        "        # Set the start time of episode k, Tk :=t.\n",
        "        Tk = t\n",
        "        \n",
        "        # For all (s,a) in S Ã—A initialize the state-action counts for episode k, Vk(s,a) := 0##\n",
        "        Vk = np.zeros((numStates, numActions))\n",
        "        \n",
        "        #pConfidenceBound = np.sqrt(np.multiply(pHat, 1-pHat) / np.clip(stateActionCounts, 1.0, None)) + (1 / np.clip(stateActionCounts, 1.0, None))\n",
        "        # Ensuring dimension of pConfidenceBound is HxSxAxS\n",
        "        temp = np.clip(stateActionCounts, 1.0, None)\n",
        "        pConfidenceBound = np.sqrt(np.multiply(pHat, 1-pHat) / np.repeat(temp[:, :, np.newaxis], pHat.shape[3], axis=pHat.shape[3])) + np.repeat((1 / temp)[:, :, np.newaxis], pHat.shape[3], axis=pHat.shape[3])\n",
        "\n",
        "        \n",
        "        PIk, Mk = extended_value_iteration(env, timesteps, stateActionCounts, pHat, pConfidenceBound, env.c, env.d, env.alpha)\n",
        "        \n",
        "        # Execute the policy\n",
        "        for h in range(timesteps):\n",
        "          action = PIk[h, state]\n",
        "          nextState, cost = env.transit(action)\n",
        "          yield (t, state, action, nextState, cost)   \n",
        "          Vk[state, action] += 1\n",
        "          stateActionCosts[state, action] += cost\n",
        "          stateActionStateCounts[state, action, nextState] += 1  \n",
        "          t += 1\n",
        "          state = nextState  \n",
        "          action = PIk[state]\n",
        "            \n",
        "          \n",
        "        pHat = Mk"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h7E_ZkyVc8t",
        "outputId": "3d7a8e2a-14cd-4007-f606-dd4f649f2d39"
      },
      "source": [
        "theta = 0.1\n",
        "numStates = 2\n",
        "numActions = 2\n",
        "p = np.array([\n",
        "    [[1, 0], [.3, .7]],\n",
        "    [[0.1, 0.9], [.2, .8]]\n",
        "])\n",
        "print('Transition matrix shape',p.shape)\n",
        "\n",
        "c = np.reshape(np.array([[.5, .9], [.3, .6]]), (numStates*numActions,1))\n",
        "d = np.reshape(np.array([\n",
        "                         [[.4, .8], [.1, .3]],\n",
        "                         [[.25, .7], [.15, .8]]\n",
        "                         ]), (2,numStates*numActions,1))\n",
        "print(d.shape)\n",
        "alpha = np.array([0.5, .2])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transition matrix shape (2, 2, 2)\n",
            "(2, 4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKtTFycbDIof",
        "scrolled": false,
        "outputId": "931b3289-c33b-480b-beb4-8b4bf087df08"
      },
      "source": [
        "experiments = []\n",
        "numSteps = 40000\n",
        "numRepeats = 20\n",
        "timesteps = 1\n",
        "NUM_EPS = 600\n",
        "for i in range(numRepeats):\n",
        "    env = MDP(numStates, numActions, p, c, d, alpha)\n",
        "    transitions = cmdp_algo(env, delta=0.1, timesteps=1, episode_length=NUM_EPS, initialState=0)\n",
        "    \n",
        "    curtExp = []\n",
        "    for j in range(NUM_EPS):\n",
        "        (t, state, action, nxtState, cost) = transitions.__next__()\n",
        "        curtExp.append((t, state, action, nxtState, cost))\n",
        "    experiments.append(curtExp)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHtf2AvMDIog"
      },
      "source": [
        "cumRegrets = np.zeros([numRepeats, numSteps])\n",
        "perStepRegrets = np.zeros([numRepeats, numSteps])\n",
        "optimalCost = 0.0\n",
        "\n",
        "for i in range(numRepeats):\n",
        "    cumRegret = 0.0\n",
        "    for j in range(NUM_EPS):\n",
        "        (t, state, action, nxtState, cost) = experiments[i][j]\n",
        "        cumRegret += cost - optimalCost\n",
        "        cumRegrets[i, j] = cumRegret\n",
        "        perStepRegrets[i, j] = cumRegret / t"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN1n-mlzDIog",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "c3bbffa1-dba3-4743-fa27-bdfa018c73d4"
      },
      "source": [
        "plt.plot(cumRegrets.mean(axis=0))\n",
        "plt.title(\"Cumulative Regrets per Step\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Cumulative Regrets per Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSFMcnsbDIoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "19972475-0a39-49a8-bc55-6fbf9daf4898"
      },
      "source": [
        "plt.plot(perStepRegrets.mean(axis=0))\n",
        "plt.title(\"Regret per Step\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Regret per Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}